{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27355c4f-3bdf-4e89-91f7-9a7203224fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q.1\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features in a dataset into a specific range, typically between 0 and 1. This scaling method is useful when you want to bring all features to a common scale so that they have equal importance during modeling, especially in algorithms that are sensitive to feature scales, such as gradient descent-based optimization algorithms.\n",
    "The formula for Min-Max scaling is as follows for a single feature:\n",
    "Xscaled = X-Xmin\n",
    "        ---------\n",
    "         Xmax-Xmin\n",
    " Where:\n",
    "X is the original feature value.\n",
    "Xscaled is the scaled feature value.\n",
    "Xmin is the minimum value of the feature in the dataset.\n",
    "Xmax is the maximum value of the feature in the dataset.       \n",
    "\n",
    "Here's how Min-Max scaling is used in data preprocessing:\n",
    "1.Identify the Features: Choose the numerical features in your dataset that you want to scale. These features should have different ranges or units.\n",
    "2.Calculate Minimum and Maximum Values: For each selected feature, calculate the minimum (Xmin) and maximum (Xmax) values within the dataset. You can do this for each feature separately.\n",
    "3.Apply Min-Max Scaling: Use the formula mentioned above to scale each feature within the specified range (usually 0 to 1). This transformation ensures that all features have values in the same scale, which can be important for machine learning algorithms that rely on distance metrics or gradient-based optimization.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51beb45e-3500-4c78-9170-6fa0e8f27ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "df=sns.load_dataset(\"titanic\") #loading titanic dataset from seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d9408fb-0d36-40ee-9746-f41e5b088763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler() #create instance of MinMaxScaler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678e7303-5706-4a81-9d78-90bd97f51c52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27117366, 0.01415106],\n",
       "       [0.4722292 , 0.13913574],\n",
       "       [0.32143755, 0.01546857],\n",
       "       ...,\n",
       "       [       nan, 0.04577135],\n",
       "       [0.32143755, 0.0585561 ],\n",
       "       [0.39683338, 0.01512699]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit_transform(df[['age','fare']]) #fit the min_max to the dataframe and transform age and fare columns in 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1c419-812c-4a4a-9260-947adb2497cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"After Min-Max scaling, both the \"Age\" and \"Fare\" features have been transformed to a range between 0 and 1. These scaled features can now be used in machine learning algorithms that require features to be on a consistent scale, ensuring that neither feature dominates the other due to differences in their original scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaacf05a-ec5a-409f-9107-78a5a863ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q.2\n",
    "The Unit Vector technique in feature scaling, also known as \"unit normalization\" or \"vector normalization,\" is a data preprocessing method used to scale numerical features to have a unit norm, typically a Euclidean norm of 1. This technique is particularly useful when you want to emphasize the direction of the data points rather than their absolute values. It's commonly used in machine learning algorithms that rely on distance calculations or vector operations, such as clustering or support vector machines (SVMs).\n",
    "The Unit Vector technique scales each data point (vector) such that its length becomes 1 (i.e., it becomes a unit vector). The formula for scaling a vector to have unit norm is as follows:\n",
    "Unit Vector = Vector\n",
    "             --------\n",
    "            ||Vector||\n",
    "Where:\n",
    "Vector is the original numerical feature vector.    \n",
    "||Vector|| represents the Euclidean norm or L2 norm of the vector, which is calculated as the square root of the sum of squared values in the vector.\n",
    "Here's how the Unit Vector technique differs from Min-Max scaling:\n",
    "\n",
    "Aspect                         Unit Vector Scaling                                                   Min-Max Scaling\n",
    "Scaling Purpose             Emphasizes direction of data points                             Standardizes feature values\n",
    "Scaling Goal                Unit norm (typically Euclidean norm)                            Range between 0 and 1\n",
    "Independence of Features    Scales each feature independently                               Scales each feature independently\n",
    "Use Cases                   Algorithms relying on direction (e.g., clustering, SVM)         Algorithms sensitive to feature scales (e.g., gradient descent, neural networks)\n",
    "Range of Scaled Values      Varies depending on data distribution; always has norm of 1     Always between 0 and 1\n",
    "Common Applications         Text data analysis, sparse data, PCA                            General data preprocessing, ensuring feature comparability\n",
    "Example                     Data points become unit vectors                                 Data points are scaled to a specified range\n",
    "This table provides a concise summary of how these two scaling techniques differ in terms of their purpose, goals, independence of features, use cases, and the range of scaled values they produce.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3d99796-8cc3-411d-8977-06aab8a652e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length_norm</th>\n",
       "      <th>sepal_width_norm</th>\n",
       "      <th>petal_length_norm</th>\n",
       "      <th>petal_width_norm</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803773</td>\n",
       "      <td>0.551609</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.031521</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828133</td>\n",
       "      <td>0.507020</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.033801</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.548312</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>0.034269</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.539151</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.034784</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.721557</td>\n",
       "      <td>0.323085</td>\n",
       "      <td>0.560015</td>\n",
       "      <td>0.247699</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.729654</td>\n",
       "      <td>0.289545</td>\n",
       "      <td>0.579090</td>\n",
       "      <td>0.220054</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.716539</td>\n",
       "      <td>0.330710</td>\n",
       "      <td>0.573231</td>\n",
       "      <td>0.220474</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.674671</td>\n",
       "      <td>0.369981</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.250281</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.350979</td>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.210588</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length_norm  sepal_width_norm  petal_length_norm  petal_width_norm  \\\n",
       "0             0.803773          0.551609           0.220644          0.031521   \n",
       "1             0.828133          0.507020           0.236609          0.033801   \n",
       "2             0.805333          0.548312           0.222752          0.034269   \n",
       "3             0.800030          0.539151           0.260879          0.034784   \n",
       "4             0.790965          0.569495           0.221470          0.031639   \n",
       "..                 ...               ...                ...               ...   \n",
       "145           0.721557          0.323085           0.560015          0.247699   \n",
       "146           0.729654          0.289545           0.579090          0.220054   \n",
       "147           0.716539          0.330710           0.573231          0.220474   \n",
       "148           0.674671          0.369981           0.587616          0.250281   \n",
       "149           0.690259          0.350979           0.596665          0.210588   \n",
       "\n",
       "     sepal_length  sepal_width  petal_length  petal_width  \n",
       "0             5.1          3.5           1.4          0.2  \n",
       "1             4.9          3.0           1.4          0.2  \n",
       "2             4.7          3.2           1.3          0.2  \n",
       "3             4.6          3.1           1.5          0.2  \n",
       "4             5.0          3.6           1.4          0.2  \n",
       "..            ...          ...           ...          ...  \n",
       "145           6.7          3.0           5.2          2.3  \n",
       "146           6.3          2.5           5.0          1.9  \n",
       "147           6.5          3.0           5.2          2.0  \n",
       "148           6.2          3.4           5.4          2.3  \n",
       "149           5.9          3.0           5.1          1.8  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "df1=sns.load_dataset(\"iris\")\n",
    "from sklearn.preprocessing import normalize\n",
    "d1=pd.DataFrame(normalize(df1[['sepal_length','sepal_width','petal_length','petal_width']]),columns=['sepal_length_norm','sepal_width_norm','petal_length_norm','petal_width_norm'])\n",
    "d2=pd.DataFrame(df1[['sepal_length','sepal_width','petal_length','petal_width']])\n",
    "pd.concat([d1,d2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640d2fa-85ab-4061-a106-91edb51d0990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Q.3\n",
    "PCA, which stands for Principal Component Analysis, is a dimensionality reduction technique used in data analysis and machine learning. Its primary goal is to reduce the dimensionality of a dataset while retaining as much relevant information as possible. PCA achieves this by transforming the original features into a new set of uncorrelated features called principal components. These principal components are ordered in such a way that the first principal component captures the most variance in the data, the second captures the second most, and so on. By selecting a subset of these principal components, you can effectively reduce the dimensionality of the data.\n",
    "Here's how PCA is used for dimensionality reduction:\n",
    "1.Compute Principal Components: PCA starts by computing the principal components of the dataset. These principal components are linear combinations of the original features. They are ordered in such a way that the first principal component captures the most variance in the data, the second captures the second most, and so on. The number of principal components is equal to the original dimensionality of the dataset.\n",
    "2.Variance Explained: PCA provides information about how much variance each principal component explains. By examining the cumulative variance explained by the principal components, you can make an informed decision about how many principal components to retain. Typically, you choose a number of components that collectively explain a high percentage of the total variance in the data. For example, you might decide to retain enough components to explain 95% of the variance.\n",
    "3.Projection: After selecting the desired number of principal components, you project the original data onto the subspace defined by these components. This projection results in a lower-dimensional representation of the data. The projected data retains the most important information while reducing dimensionality.\n",
    "4.Reduced-Dimension Dataset: The reduced-dimension dataset is now suitable for further analysis or modeling. It has fewer features than the original dataset, which can lead to benefits such as reduced computational complexity, improved model generalization, and easier data visualization.\n",
    "\n",
    "Suppose we have a high-dimensional dataset with 100 features (columns). We want to reduce its dimensionality to 20 features while preserving as much relevant information as possible. Here's how we would use PCA for this task:\n",
    "1.Compute the principal components for the entire dataset.\n",
    "2.Examine the cumulative explained variance to decide how many principal components to retain. Suppose we find that the first 20 principal components collectively explain 95% of the total variance.\n",
    "3.Project the original dataset onto the subspace defined by these 20 principal components.\n",
    "4.The resulting dataset has 20 features and is a lower-dimensional representation of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9164859-6fae-480e-85af-bf06ca86a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q.4\n",
    "PCA (Principal Component Analysis) is closely related to feature extraction, and it can be used as a feature extraction technique. The relationship between PCA and feature extraction lies in the fact that PCA transforms the original features into a set of new features (principal components) that are linear combinations of the original features. These principal components are ordered by the amount of variance they capture, making them a valuable representation of the data.\n",
    "Here's how PCA can be used for feature extraction and its relationship to the concept:\n",
    "1.Dimensionality Reduction: PCA is often used to reduce the dimensionality of a dataset while retaining most of its relevant information. By selecting a subset of the top-ranked principal components, you effectively extract a reduced set of features that capture the most significant variability in the data.\n",
    "2.Feature Ranking: Principal components are ranked by the amount of variance they explain. The first principal component captures the most variance, the second captures the second most, and so on. This ranking allows you to prioritize the most informative features.\n",
    "3.Reduced Features: The selected principal components serve as a new feature set, which is typically lower in dimensionality than the original feature space. These components are often used as features for subsequent analysis, modeling, or visualization.\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "Suppose you have a dataset of images, each represented as a vector of pixel values. Each pixel represents a feature, and the high dimensionality of the pixel values makes the dataset challenging to work with. You want to reduce the dimensionality while preserving the essential information in the images.\n",
    "In this example:\n",
    "*We generate a synthetic dataset representing 2D images, where each row is an image with 400 pixel features (20x20 pixels).\n",
    "*We initialize a PCA object to reduce the dimensionality of the dataset to a lower dimension, in this case, 10 principal components.\n",
    "*We fit the PCA model to the data and transform the data to obtain a reduced feature set with only 10 features. These 10 features are linear combinations of the original pixel values and capture the most significant variation in the images.\n",
    "*The data_reduced variable now contains the reduced feature representations of the images, which can be used for various tasks such as image classification or visualization.\n",
    "PCA, in this context, serves as a feature extraction technique that reduces the dimensionality of the dataset while retaining essential information, making it more suitable for subsequent analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1469e753-5c63-4df5-826d-b09bac56b405",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.28571429, 0.5       ],\n",
       "       [0.2       , 0.68571429, 1.        ],\n",
       "       [0.4       , 1.        , 0.16666667],\n",
       "       [0.6       , 0.6       , 1.        ],\n",
       "       [0.8       , 0.8       , 0.        ],\n",
       "       [1.        , 0.        , 0.66666667]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q.5\n",
    "import pandas as pd\n",
    "data=pd.DataFrame({    # Create a dataset contains price,rating,delivery time\n",
    "    'Price':[10,20,30,40,50,60],\n",
    "    'Rating':[2.5,3.9,5.0,3.6,4.3,1.5],\n",
    "    'Delivery Time (min)':[30,45,20,45,15,35]\n",
    "})\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()   # Initialize the min_max\n",
    "min_max.fit_transform(data) # Fit and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce799c1e-b481-4cc3-ae4b-4b2148c4198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q.6\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis to reduce the complexity of high-dimensional datasets while preserving as much relevant information as possible. Here's how you can use PCA to reduce the dimensionality of a dataset for predicting stock prices:\n",
    "1.Data Collection and Preprocessing:\n",
    "Collect your dataset, which should include features related to company financial data, market trends, and other relevant factors.\n",
    "Perform data preprocessing, including handling missing values, normalizing or standardizing data, and encoding categorical variables if necessary.\n",
    "2.Feature Selection and Engineering:\n",
    "Identify the features that are relevant to predicting stock prices. These may include financial indicators (e.g., revenue, earnings, debt), market indicators (e.g., S&P 500 index),and any other factors believed to influence stock prices.\n",
    "Perform feature engineering if needed, creating new features or transformations that might capture valuable information.\n",
    "3.Standardization:\n",
    "Standardize the features to have a mean of 0 and a standard deviation of 1. Standardization is essential for PCA, as it ensures that all features contribute equally to the analysis, regardless of their scales.\n",
    "4.PCA Application:\n",
    "Apply PCA to the standardized dataset to reduce dimensionality while retaining as much variance as possible. PCA accomplishes this by finding linear combinations of the original features, called principal components, that capture the most significant variance in the data.\n",
    "Specify the desired number of principal components to retain. You can decide based on the amount of variance explained (e.g., retaining 95% of the variance) or domain knowledge.\n",
    "5.Component Analysis:\n",
    "Analyze the explained variance ratio to understand how much information each principal component retains. Plotting the cumulative explained variance can help you decide how many components to keep.\n",
    "6.Dimensionality Reduction:\n",
    "Select the top-k principal components that retain most of the variance. These components represent a reduced set of features.\n",
    "Transform your original dataset by projecting it onto the selected principal components.\n",
    "7.Model Building:\n",
    "Use the reduced-dimension dataset for training your stock price prediction model. Common algorithms include linear regression, support vector machines, neural networks, or time series models like ARIMA or LSTM, depending on the nature of your problem.\n",
    "8.Evaluation and Fine-Tuning:\n",
    "Evaluate your model's performance using appropriate metrics (e.g., Mean Absolute Error, Root Mean Squared Error) and fine-tune hyperparameters as needed.\n",
    "9.Prediction:\n",
    "Make stock price predictions using new data based on the trained model.\n",
    "10.Monitoring and Updating:\n",
    "Continuously monitor the model's performance and update it as new data becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13c19c6c-abd2-45af-bd4b-06b9d5c48ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q.7\n",
    "import pandas as pd\n",
    "data=[1,5,10,15,20]\n",
    "df=pd.DataFrame(data)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()   # Initialize the min_max\n",
    "min_max.fit_transform(df) # Fit and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f49c39-fa9f-4f07-b4ba-6dec1e29a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q.8\n",
    "The decision of how many principal components to retain in PCA depends on several factors, including the goals of your analysis, the amount of variance you want to preserve, and the trade-off between dimensionality reduction and information retention. Here are steps you can follow to determine the number of principal components to retain:\n",
    "1.Standardization: Start by standardizing the numerical features (height, weight, age, and blood pressure) to have a mean of 0 and a standard deviation of 1. PCA is sensitive to the scale of features, so standardization is crucial.\n",
    "2.Covariance Matrix: Calculate the covariance matrix of the standardized features. This matrix represents the relationships between the features.\n",
    "3.Eigenvalues and Eigenvectors: Compute the eigenvalues and eigenvectors of the covariance matrix. These will help you understand the variance explained by each principal component and the direction of the components in the original feature space.\n",
    "4.Explained Variance Ratio: Calculate the explained variance ratio for each principal component. This ratio represents the proportion of total variance explained by each component. It is essential to decide how much variance you want to retain.\n",
    "5.Cumulative Explained Variance: Create a plot of the cumulative explained variance as you add more principal components. This plot helps you determine the number of components that explain a satisfactory amount of variance. A common threshold is to retain enough components to explain, for example, 95% of the total variance.\n",
    "6.Select the Number of Components: Based on the cumulative explained variance plot and your specific goals, choose the number of principal components to retain. If you want to retain most of the variance while reducing dimensionality, you may choose a number that explains a high percentage of the variance (e.g., 95%).\n",
    "7.PCA Transformation: Transform your dataset by projecting it onto the selected principal components.\n",
    "8.Model Building: Use the reduced-dimension dataset for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549caf9-9282-4cf1-bb57-0a9f6fba5c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
